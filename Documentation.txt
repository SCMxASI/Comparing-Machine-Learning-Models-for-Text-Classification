"20 Newsgroups Text Classification Project"

1.Overview:
--This project uses the 20 Newsgroups dataset to train and evaluate several machine learning models for text                    classification. The goal is to classify news articles into their respective categories.

2.Dataset:
--The 20 Newsgroups dataset is a collection of approximately 20,000 newsgroup documents, partitioned across 20       different newsgroups. The dataset is widely used for text classification and clustering tasks.

3.Project Structure:
--The project consists of the following components:

 ➖  import statements for necessary libraries
 ➖  Data loading and preprocessing
 ➖ Model training and evaluation
 ➖  Hyperparameter tuning using GridSearchCV
 ➖ Comparison of different machine learning models

4.Code:
//python
//Copy code
import numpy as np
import pandas as pd
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.utils.class_weight import compute_class_weight
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
import seaborn as sns
import matplotlib.pyplot as plt

# Load the 20 Newsgroups dataset
dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))

# Create a pandas dataframe
df = pd.DataFrame({'text': dataset.data, 'label': dataset.target})

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.2, random_state=42)

# Create a TF-IDF vectorizer with hyperparameter tuning
vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7, min_df=0.1)

# Define a pipeline with the vectorizer and a classifier
pipeline = Pipeline([
    ('vectorizer', vectorizer),
    ('classifier', MultinomialNB())
])

# Define hyperparameter tuning space
param_grid = {
    'vectorizer__max_df': [0.5, 0.6, 0.7, 0.8, 0.9],
    'vectorizer__min_df': [0.05, 0.1, 0.15, 0.2],
    'classifier__alpha': [0.1, 0.5, 1.0, 5.0, 10.0]
}

# Perform grid search with cross-validation
grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='f1_macro')
grid_search.fit(X_train, y_train)

# Get the best-performing model
best_model = grid_search.best_estimator_

# Make predictions on the testing data
y_pred = best_model.predict(X_test)

# Evaluate the model
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Classification Report:")
print(classification_report(y_test, y_pred))
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

# Plot the confusion matrix as a heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, cmap="YlGnBu")
plt.xlabel("Predicted labels")
plt.ylabel("True labels")
plt.title("Confusion Matrix")
plt.show()

# Compare with other models
models = [
    ('Logistic Regression', LogisticRegression()),
    ('Random Forest', RandomForestClassifier()),
    ('Support Vector Machine', SVC())
]

for name, model in models:
    pipeline = Pipeline([
        ('vectorizer', vectorizer),
        ('classifier', model)
    ])
    pipeline.fit(X_train, y_train)
    y_pred = pipeline.predict(X_test)
    print(f"{name} Accuracy:", accuracy_score(y_test, y_pred))
    print(f"{name} Classification Report:")
    print(classification_report(y_test, y_pred))
    print(f"{name} Confusion Matrix:")
    print(confusion_matrix(y_test, y_pred))

    # Plot the confusion matrix as a heatmap
    plt.figure(figsize=(10, 8))
    sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, cmap="YlGnBu")
    plt.xlabel("Predicted labels")
    plt.ylabel("True labels")
    plt.title(f"{name} Confusion Matrix")
    plt.show()

5.Usage:
➖Install the required libraries by running pip install -r requirements.txt (assuming you have a requirements.txt file with the necessary libraries).
➖Run the script using Python (e.g., python script.py).
➖The script will load the 20 Newsgroups dataset, preprocess the data, train and evaluate several machine learning models, and compare their performance.

6.License:
--This project is licensed under the MIT License.

7.Contributing:
--Contributions are welcome! If you'd like to contribute to this project, please fork the repository, make your changes, and submit a pull request.

7.Troubleshooting:
--If you encounter any issues while running the script, please check the following:

➖Ensure you have the necessary libraries installed.
➖Verify that the dataset is loaded correctly.
➖Check the hyperparameter tuning space and adjust it if necessary.

                                                                            Thank you!
